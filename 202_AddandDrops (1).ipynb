{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from urllib import FancyURLopener  # This is library that helps us create the headless browser\n",
    "import urllib2\n",
    "from random import choice #This library helps pick a random item from a list\n",
    "import bs4 as bs\n",
    "import datetime\n",
    "from datetime import timedelta, date\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import sys\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "    'Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16',\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 6.1; rv:2.2) Gecko/20110201',\n",
    "    'Mozilla/5.0 (Macintosh; Intel MaPc OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246'\n",
    "]\n",
    "\n",
    "class MyOpener(FancyURLopener, object):\n",
    "    version = choice(user_agents)\n",
    "\n",
    "def getDate(i):\n",
    "    return pd.to_datetime(date,infer_datetime_format=True)\n",
    "\n",
    "def getCompanyNames(link):\n",
    "    Companies = []\n",
    "    myopener = MyOpener()\n",
    "    page=myopener.open('https://www.sec.gov'+link)\n",
    "    \n",
    "    html = page.read()\n",
    "    soup = bs.BeautifulSoup(html, 'lxml')\n",
    "    infotable = soup.find_all(\"table\", summary = \"Form 13F-NT Header Information\")\n",
    "    rows = infotable[0].find_all('tr')\n",
    "    for row in rows[3:]:\n",
    "        cells = row.find_all('td')\n",
    "        Companies.append(cells[0].getText().encode('utf-8'))\n",
    "    return Companies\n",
    "\n",
    "def getData(NBList):\n",
    "    name_of_issuer = []\n",
    "    NBList = filter(None, NBList.split('\\n'))\n",
    "    for i in range(2,len(NBList)):\n",
    "        cells = re.split(r'\\s{2,}', NBList[i])\n",
    "        name_of_issuer.append(cells[0])\n",
    "    return name_of_issuer\n",
    "            \n",
    "def getCompanyNamesTXT(link):\n",
    "    CompanyNames = []\n",
    "    html = urllib2.urlopen('https://www.sec.gov'+link).read()\n",
    "    html = html.upper()\n",
    "    html = html.replace('<S>', '  ')\n",
    "    html = html.replace('<C>', '  ')\n",
    "    index = html.find('FORM 13F INFORMATION')\n",
    "    html2 = html[index:]\n",
    "    index = html2.find('NAME OF ISSUER')\n",
    "    html2 = html2[index:]\n",
    "    while(html2.find('NAME OF ISSUER') != -1):\n",
    "        start = html2.find('NAME OF ISSUER')\n",
    "        remaining = html2[start:]\n",
    "        end = html2.find('</TABLE>')\n",
    "        CompanyNames.extend(getData(html2[start:end]))\n",
    "        html2 = remaining[end:]\n",
    "    return CompanyNames\n",
    "\n",
    "def getCompanies(link):\n",
    "    Companies = []\n",
    "    if(link[-4:]=='.xml'):\n",
    "        Companies = getCompanyNames(link)\n",
    "    if(link[-4:]=='.txt'):\n",
    "        Companies = getCompanyNamesTXT(link)\n",
    "    return Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0000772129', '0000813470', '0001000207', '0001311981'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib2\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "CIK = []\n",
    "text_link = []\n",
    "text_Name = []\n",
    "text_date = []\n",
    "\n",
    "input_file = open('C:\\Users\\Nick\\Documents\\Current Courses\\Web Data Analytics\\WDA-Assignment 1\\Assignment_1\\Assignment_1/HW_Mutual_Fund_Info_Table_Link.csv', 'r')\n",
    "\n",
    "rows = input_file.readlines()\n",
    "input_file.close()\n",
    "\n",
    "# Your Code on Enumerating the Lists.  The result should be three lists, text_link, text_Name,\n",
    "#and text_date.  Each should have length 122.\n",
    "\n",
    "# Since we have a comma separated file, I created a dataframe out of the rows.\n",
    "df = pd.DataFrame([row.split(\",\") for row in rows[1:]])\n",
    "\n",
    "# I used the first row for column headers for the aforementioned dataframe.\n",
    "df.columns = rows[0].split(\", \")\n",
    "\n",
    "# Since every row has a end of line character as a column, I removed them here\n",
    "del df['\\n']\n",
    "\n",
    "# Reset the index since the indexes are messed up after xml files are dropped.\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "df1=df.groupby(['CIK','Name'], as_index=False)['Date'].count()\n",
    "df1=df1[df1['Date']>=2]\n",
    "\n",
    "CIKdropList = ['0000711202', '0000003521', '0000002110', '0000777535', '0001029068','0000355437']\n",
    "\n",
    "df = df[(df['CIK'].isin(df1['CIK'].tolist()))]\n",
    "df = df[~df['CIK'].isin(CIKdropList)]\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "df = df.sort_values('Date').groupby('CIK').tail(2)\n",
    "df = df.reset_index(drop = True)\n",
    "# Populate the required lists using the respective columns from the data frame.\n",
    "CIK = df['CIK'].tolist()\n",
    "xml_link = df['Information Table Links'].tolist()\n",
    "xml_Name = df['Name'].tolist()\n",
    "xml_date = df['Date'].tolist()\n",
    "\n",
    "df = df[['CIK', 'Name', 'Information Table Links', 'File Type']]\n",
    "set(df['CIK'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 41\n"
     ]
    }
   ],
   "source": [
    "CompanyList = list(set(df['Name'].tolist()))\n",
    "\n",
    "for i in CompanyList:\n",
    "    df_new = df.loc[df['Name']==CompanyList[1]]\n",
    "    InfoLinks = df_new['Information Table Links'].tolist()\n",
    "    OldCompanies = getCompanies(InfoLinks[0])\n",
    "    NewCompanies = getCompanies(InfoLinks[1])\n",
    "    added = set(NewCompanies) - set(OldCompanies)\n",
    "    dropped = set(OldCompanies) - set(NewCompanies)\n",
    "\n",
    "print len(added), len(dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " 'AERCAP HOLDINGS NV',\n",
       " 'AEROPOSTALE',\n",
       " 'ALBANY MOLECULAR RESH INC',\n",
       " 'ALCATEL-LUCENT',\n",
       " 'ALTEVA',\n",
       " 'ASSURED GUARANTY LTD',\n",
       " 'BEBE STORES INC',\n",
       " 'BROADRIDGE FINL SOLUTIONS IN',\n",
       " 'CAL MAINE FOODS INC',\n",
       " 'CHRISTOPHER & BANKS CORP',\n",
       " 'COCA COLA BOTTLING CO CONS',\n",
       " 'COEUR D ALENE MINES CORP IDA',\n",
       " 'COMVERSE INC',\n",
       " 'COSI INC',\n",
       " 'COWEN GROUP INC NEW',\n",
       " 'DOLE FOOD CO INC NEW',\n",
       " 'EMULEX CORP',\n",
       " 'GENWORTH FINL INC COM',\n",
       " 'GFI GROUP INC',\n",
       " 'HARDINGE INC',\n",
       " 'IAC INTERACTIVECORP',\n",
       " 'INTRALINKS HLDGS INC',\n",
       " 'JOURNAL COMMUNICATIONS INC',\n",
       " 'KEYNOTE SYS INC',\n",
       " 'KROGER CO',\n",
       " 'MATRIX SVC CO',\n",
       " 'MONTPELIER RE HOLDINGS LTD',\n",
       " 'MYERS INDS INC',\n",
       " 'NN INC',\n",
       " 'OMEGA PROTEIN CORP',\n",
       " 'PNM RES INC',\n",
       " 'REAL GOODS SOLAR INC',\n",
       " 'SPRINT NEXTEL CORP',\n",
       " 'TELLABS INC',\n",
       " 'TETRA TECHNOLOGIES INC DEL',\n",
       " 'TRIQUINT SEMICONDUCTOR INC',\n",
       " 'UNS ENERGY CORP',\n",
       " 'VASCULAR SOLUTIONS INC',\n",
       " 'VISHAY INTERTECHNOLOGY INC COM COM',\n",
       " 'YAHOO INC'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "resultFyle = open(\"1000207add\",'w')\n",
    "OutputFile = open(\"1000207drop\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for r in list(added):\n",
    "    resultFyle.write(r + \"\\n\")\n",
    "resultFyle.close()\n",
    "    \n",
    "for r in list(dropped):\n",
    "    OutputFile.write(r + \"\\n\")\n",
    "OutputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ONLY NEEDED IF YOU ARE SCRAPING THE LINKS OF THE CIK. OUR ASSIGNMENT 1 HAS ALL THE LINKS FROM THE CIKs\n",
    "CIK = ['0000002110',\n",
    "       '0000003521',\n",
    "       '0000355437',\n",
    "       '0000711202',\n",
    "       '0000749155',\n",
    "       '0000772129',\n",
    "       '0000777535', \n",
    "       '0000797136',\n",
    "       '0000813470',\n",
    "       '0000917124',\n",
    "       '0001000207',\n",
    "       '0001002556',\n",
    "       '0001029068',\n",
    "       '0001157756',\n",
    "       '0001311981', \n",
    "       '0001432353']\n",
    "Urls = []\n",
    "for i in range(0, len(CIK)):# \n",
    "    myopener = MyOpener()\n",
    "    page=myopener.open('https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK='+CIK[i]+'&type=N-Q&dateb=&count=100&scd=filings')\n",
    "    \n",
    "    html = page.read()\n",
    "    soup = bs.BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    dateText = soup.find_all('td', class_='small')\n",
    "    for i in range(0,len(dateText)):\n",
    "        date = dateText[i].find_next_sibling('td').getText()\n",
    "        date = getDate(date)\n",
    "        if (date >= datetime.datetime(2016,9,1)):\n",
    "            DocumentLink = soup.find_all('a', id = 'documentsbutton')\n",
    "            Urls.append('https://www.sec.gov' + DocumentLink[i]['href'])\n",
    "Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "date=[]\n",
    "info_table = []\n",
    "info_type = []\n",
    "\n",
    "for page in Urls:\n",
    "    html = urllib2.urlopen(page).read()   \n",
    "    date_index = html.find('div class=\\\"infoHead\\\">Filing Date<')\n",
    "    date_html = html[date_index:]\n",
    "    date_index2 = date_html.find('class=\\\"info\\\">')\n",
    "    date_html = date_html[date_index2:]\n",
    "    start = date_html.find(\">\")\n",
    "    end = date_html.find(\"<\")\n",
    "    date.append(date_html[start+1:end])  # This gets us the date    \n",
    "    if html.find('td scope=\\\"row\\\">2<') != -1: # By this condition we check whether there is a xml version. For an xml version there are two items present in each table in the page        \n",
    "        index = html.find('td scope=\\\"row\\\">2<')\n",
    "        html2 = html[index:]    \n",
    "        start = html2.find('a href=\\\"/Archives/edgar/data/')\n",
    "        remaining = html2[start:]\n",
    "        end = remaining.find('>')    \n",
    "        info_table.append(remaining[8:end-1]) # we collect the link    \n",
    "        info_type.append('xml') # we mention that this is an xml link            \n",
    "    else: # Or else, we get the idea that the info table is in text format        \n",
    "        index = html.find('td scope=\\\"row\\\">Complete submission text file')\n",
    "        html2 = html[index:]        \n",
    "        start = html2.find('a href=\\\"/Archives/edgar/data/')\n",
    "        remaining = html2[start:]\n",
    "        end = remaining.find('>')    \n",
    "        info_table.append(remaining[8:end-1]) \n",
    "        info_type.append('text') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
