{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from urllib import FancyURLopener  # This is library that helps us create the headless browser\n",
    "from random import choice #This library helps pick a random item from a list\n",
    "import bs4 as bs\n",
    "import datetime\n",
    "from datetime import timedelta, date\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import sys\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "    'Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16',\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 6.1; rv:2.2) Gecko/20110201',\n",
    "    'Mozilla/5.0 (Macintosh; Intel MaPc OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246'\n",
    "]\n",
    "\n",
    "class MyOpener(FancyURLopener, object):\n",
    "    version = choice(user_agents)\n",
    "\n",
    "# Get each article and their link\n",
    "def getArticleMeta(infotable):\n",
    "    for i in range(0,len(infotable)):\n",
    "        CallDesc.append(infotable[i].getText().encode('utf-8'))\n",
    "        CallLink.append(infotable[i]['href'])\n",
    "        \n",
    "def getDate(i):\n",
    "    date = i.find(\"span\", class_= \"date\")\n",
    "    date = date.getText()\n",
    "    try:\n",
    "        date = pd.to_datetime(date,infer_datetime_format=True)\n",
    "    except ValueError:\n",
    "        if \"Yesterday\" in date:\n",
    "            Date = datetime.date.today() - timedelta(1)\n",
    "            date = date[10:]\n",
    "            date = str(Date) + date\n",
    "        else:\n",
    "            date = date.split(',')[1]\n",
    "            date = date + ' ' + str(2017)\n",
    "        date = pd.to_datetime(date,infer_datetime_format=True)\n",
    "    return date\n",
    "\n",
    "def getTicker(companyName):\n",
    "    df = pd.read_csv('company.txt', sep = '\\t')\n",
    "    df = df[df['Country'] == 'USA']\n",
    "    dfD = df.set_index('Name')['Ticker'].to_dict()\n",
    "    del df\n",
    "    return dfD[process.extractOne(companyName, dfD.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This uses the search tool on Seeking Alpha to get pull specific company's transcript.\n",
    "\n",
    "def getTranscriptBySearch(companyName):\n",
    "    CallDesc = []\n",
    "    CallLink = []\n",
    "    company = companyName\n",
    "    driver = webdriver.Firefox(executable_path='/Users/sumerdhillon/Dropbox/MS-BAIM/Second Module/MGMT 590/Lecture 7/geckodriver')\n",
    "    try:\n",
    "        ticker = getTicker(company)\n",
    "        url = 'https://seekingalpha.com/search/transcripts?term='+ ticker\n",
    "        driver.get(url) # Start the browser and open 'url'\n",
    "    except KeyError:\n",
    "        print \"That company doesn't exist in my masterlist\"\n",
    "        raise\n",
    "\n",
    "    assert 'Seeking Alpha' in driver.title # Wait for the page to load\n",
    "    html = driver.page_source # Get the html of the page\n",
    "    driver.quit() # Close the browser\n",
    "\n",
    "    soup = bs.BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    infotable = soup.find_all(\"div\", class_= \"transcript_link\")\n",
    "    for i in infotable:\n",
    "        date = getDate(i)\n",
    "        if (date >= datetime.datetime(2016,9,1)) & (ticker in i.find('a').getText()):\n",
    "            CallDesc.append(i.find('a').getText().encode('utf-8'))\n",
    "            CallLink.append(i.find('a')['href'])\n",
    "    CreateTextFiles(CallLink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This goes into the transcript link and extracts the text from each link\n",
    "def CreateTextFiles(CallLink):\n",
    "    for link in range(0,len(CallLink)):\n",
    "        myopener = MyOpener()\n",
    "        page=myopener.open('https://seekingalpha.com' + CallLink[link])\n",
    "\n",
    "        html = page.read()\n",
    "        soup = bs.BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        infotable = soup.find_all(\"p\", class_= \"p\")\n",
    "        try:\n",
    "            Output_File = open(str(infotable[0].find(\"a\")['title']) + \n",
    "                               str(infotable[2].getText()) + \".txt\",'w')\n",
    "        except IndexError as exc:\n",
    "            continue\n",
    "        except TypeError as typ:\n",
    "            continue\n",
    "\n",
    "        for i in range(0,len(infotable)):\n",
    "            if(infotable[i].getText().encode('utf-8').startswith('Copyright policy:')):\n",
    "                break\n",
    "            if(len(infotable[i].getText()) == 0):\n",
    "                time.sleep(600)\n",
    "                continue\n",
    "            Output_File.write(infotable[i].getText().encode('utf-8'))\n",
    "            Output_File.write(\"\\n\")\n",
    "        Output_File.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('AdvanceCapital.csv', usecols = ['company_name'])\n",
    "companies = df['company_name'].tolist()\n",
    "for company in companies:\n",
    "    counter = 0\n",
    "    getTranscriptBySearch(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "getTranscriptBySearch('Whirlpool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This pulls data from 1 single page of the earnings call transcript\n",
    "CallDesc = []\n",
    "CallLink = []\n",
    "\n",
    "myopener = MyOpener()\n",
    "page=myopener.open('https://seekingalpha.com/earnings/earnings-call-transcripts')\n",
    "\n",
    "html = page.read()\n",
    "soup = bs.BeautifulSoup(html, 'lxml')\n",
    "\n",
    "infotable = soup.find_all(\"a\", class_= \"dashboard-article-link\")\n",
    "for i in range(0,len(infotable)):\n",
    "    CallDesc.append(infotable[i].getText().encode('ascii'))\n",
    "    CallLink.append(infotable[i]['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This counts the number of pages of transcripts there are on the website.\n",
    "myopener = MyOpener()\n",
    "page=myopener.open('https://seekingalpha.com/earnings/earnings-call-transcripts')\n",
    "\n",
    "html = page.read()\n",
    "soup = bs.BeautifulSoup(html, 'lxml')\n",
    "\n",
    "dots = soup.find(\"li\", class_= \"dots\")\n",
    "number = dots.find_next_siblings(\"li\")\n",
    "number = number[0].find(\"a\").getText()\n",
    "\n",
    "print number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell tries to go through each page and do the same as before(get the links and names)\n",
    "# Note: There is a chance of the website detecting and stopping you after a while\n",
    "\n",
    "CallDesc = []\n",
    "CallLink = []\n",
    "counter = 0\n",
    "        \n",
    "# Open the site for the first time\n",
    "myopener = MyOpener()\n",
    "page=myopener.open('https://seekingalpha.com/earnings/earnings-call-transcripts')\n",
    "html = page.read()\n",
    "soup = bs.BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Find the total number of pages on the site at this moment\n",
    "dots = soup.find(\"li\", class_= \"dots\")\n",
    "number = dots.find_next_siblings(\"li\")\n",
    "number = number[0].find(\"a\").getText()\n",
    "\n",
    "# Now loop through each page\n",
    "for i in range(0,int(number)):\n",
    "    myopener = MyOpener()\n",
    "    page=myopener.open('https://seekingalpha.com/earnings/earnings-call-transcripts/' + str(i+1))\n",
    "    html = page.read()\n",
    "    soup = bs.BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    counter += 1\n",
    "    infotable = soup.find_all(\"a\", class_= \"dashboard-article-link\")\n",
    "    print i+1, len(infotable)\n",
    "    getArticleMeta(infotable)\n",
    "    time.sleep(choice(range(5,10)))\n",
    "    \n",
    "    if counter == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "CallDesc = []\n",
    "CallLink = []\n",
    "\n",
    "myopener = MyOpener()\n",
    "page=myopener.open('https://seekingalpha.com/earnings/earnings-call-transcripts/2')\n",
    "\n",
    "html = page.read()\n",
    "soup = bs.BeautifulSoup(html, 'lxml')\n",
    "\n",
    "\n",
    "infotable = soup.find_all(\"a\", class_= \"dashboard-article-link\")\n",
    "\n",
    "\n",
    "for i in range(0,len(infotable)):\n",
    "    CallDesc.append(infotable[i].getText().encode('utf-8'))\n",
    "    CallLink.append(infotable[i]['href'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
